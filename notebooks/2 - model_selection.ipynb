{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and fit\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = '../data/processed/SplitAndScaled.pkl'\n",
    "\n",
    "with open(DataPath, 'rb') as f:\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the amount of features with PCA\n",
    "PCA30 = PCA(n_components=30)\n",
    "X_train_reduced = PCA30.fit_transform(X_train_scaled)\n",
    "X_test_reduced = PCA30.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations\n",
    "NumObs = len(y_test_scaled)\n",
    "\n",
    "# number of predictors\n",
    "k_reduced = X_train_reduced.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.617378085562288, R2: 0.47833431184817343, AdjR2: 0.43920938523678643\n"
     ]
    }
   ],
   "source": [
    "# fitting linear regression to training data\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(X_train_reduced,y_train_scaled)\n",
    "\n",
    "# predict the test set results\n",
    "LinReg_y_pred = LinReg.predict(X_test_reduced)\n",
    "\n",
    "# check accuracy\n",
    "LinReg_MSE = mean_squared_error(y_test_scaled, LinReg_y_pred)\n",
    "LinReg_RMSE = np.sqrt(LinReg_MSE)\n",
    "LinRegR2 = r2_score(y_test_scaled, LinReg_y_pred)\n",
    "LinRegR2Adj = 1 - (1 - LinRegR2) * (NumObs - 1) / (NumObs - k_reduced - 1)\n",
    "\n",
    "print(f'RMSE: {LinReg_RMSE}, R2: {LinRegR2}, AdjR2: {LinRegR2Adj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3088262697083732, R2: -1.3445195557316607, AdjR2: -1.5203585224115352\n"
     ]
    }
   ],
   "source": [
    "# fitting polynomial regression to training data\n",
    "PolReg = PolynomialFeatures(degree = 2) \n",
    "X_Polynomial = PolReg.fit_transform(X_train_reduced)\n",
    "\n",
    "LinRegForP = LinearRegression()\n",
    "LinRegForP.fit(X_Polynomial, y_train_scaled)\n",
    "\n",
    "# predict the test set results\n",
    "PolReg_y_pred = LinRegForP.predict(PolReg.fit_transform(X_test_reduced))\n",
    "\n",
    "# check accuracy\n",
    "PolReg_MSE = mean_squared_error(y_test_scaled, PolReg_y_pred)\n",
    "PolReg_RMSE = np.sqrt(PolReg_MSE)\n",
    "PolRegR2 = r2_score(y_test_scaled, PolReg_y_pred)\n",
    "PolRegR2Adj = 1 - (1 - PolRegR2) * (NumObs - 1) / (NumObs - k_reduced - 1)\n",
    "\n",
    "print(f'RMSE: {PolReg_RMSE}, R2: {PolRegR2}, AdjR2: {PolRegR2Adj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6008274902635067, R2: 0.5059289088152876, AdjR2: 0.46887357697643417\n"
     ]
    }
   ],
   "source": [
    "# instantiate Random Forest Regressor\n",
    "RFR = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# reshape y_train_scaled to a 1D array\n",
    "reshape_y_train_scaled = y_train_scaled.ravel()\n",
    "\n",
    "# fit the model\n",
    "RFR.fit(X_train_reduced, reshape_y_train_scaled)\n",
    "\n",
    "# predict the test set results\n",
    "RFR_y_pred = RFR.predict(X_test_reduced)\n",
    "\n",
    "# check accuracy\n",
    "RFR_MSE = mean_squared_error(y_test_scaled,RFR_y_pred)\n",
    "RFR_RMSE = np.sqrt(RFR_MSE)\n",
    "RFR_R2 = r2_score(y_test_scaled, RFR_y_pred)\n",
    "RFR_R2Adj = 1 - (1 - RFR_R2) * (NumObs - 1) / (NumObs - k_reduced - 1)\n",
    "\n",
    "print(f'RMSE: {RFR_RMSE}, R2: {RFR_R2}, AdjR2: {RFR_R2Adj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6101807113685882, R2: 0.49042653737384456, AdjR2: 0.4522085276768829\n"
     ]
    }
   ],
   "source": [
    "# instantiate XGBoost model\n",
    "XGB_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)\n",
    "\n",
    "# fit the model\n",
    "XGB_model.fit(X_train_reduced, y_train_scaled)\n",
    "\n",
    "# predict the test set results\n",
    "XGB_y_pred = XGB_model.predict(X_test_reduced)\n",
    "\n",
    "# check accuracy\n",
    "XGB_MSE = mean_squared_error(y_test_scaled, XGB_y_pred)\n",
    "XGB_RMSE = np.sqrt(XGB_MSE)\n",
    "XGB_R2 = r2_score(y_test_scaled, XGB_y_pred)\n",
    "XGB_R2Adj = 1 - (1 - XGB_R2) * (NumObs - 1) / (NumObs - k_reduced - 1)\n",
    "\n",
    "print(f'RMSE: {XGB_RMSE}, R2: {XGB_R2}, AdjR2: {XGB_R2Adj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models\n",
    "MainPath = '../data/model/'\n",
    "\n",
    "# Linear Regression\n",
    "LinRegPath = 'LinReg_model.sav'\n",
    "with open(MainPath + LinRegPath, 'wb') as f:\n",
    "    pickle.dump((LinReg, X_train_reduced, X_test_reduced, y_train_scaled, y_test_scaled), f)\n",
    "\n",
    "# Polynomial Regression\n",
    "PolRegPath = 'PolReg_model.sav'\n",
    "with open(MainPath + PolRegPath, 'wb') as f:\n",
    "    pickle.dump((LinRegForP, X_train_reduced, X_test_reduced, y_train_scaled, y_test_scaled), f)\n",
    "\n",
    "# Random Forest Regresson\n",
    "RFRPath = 'RFR_model.sav'\n",
    "with open(MainPath + RFRPath, 'wb') as f:\n",
    "    pickle.dump((RFR, X_train_reduced, X_test_reduced, y_train_scaled, y_test_scaled), f)\n",
    "\n",
    "# XGBoost\n",
    "XGBPath = 'XGB_model.sav'\n",
    "with open(MainPath + XGBPath, 'wb') as f:\n",
    "    pickle.dump((XGB_model, X_train_reduced, X_test_reduced, y_train_scaled, y_test_scaled), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinReg_RMSE: 0.617378085562288, LinRegR2: 0.47833431184817343, LinRegR2Adj: 0.43920938523678643\n",
      "PolReg_RMSE: 1.3088262697083732, PolRegR2: -1.3445195557316607, PolRegR2Adj: -1.5203585224115352\n",
      "RFR_RMSE: 0.6008274902635067, RFR_R2: 0.5059289088152876, RFR_R2Adj: 0.46887357697643417\n",
      "XGB_RMSE: 0.6101807113685882, XGB_R2: 0.49042653737384456, XGB_R2Adj: 0.4522085276768829\n"
     ]
    }
   ],
   "source": [
    "# gather evaluation metrics and compare results\n",
    "print(f'LinReg_RMSE: {LinReg_RMSE}, LinRegR2: {LinRegR2}, LinRegR2Adj: {LinRegR2Adj}')\n",
    "print(f'PolReg_RMSE: {PolReg_RMSE}, PolRegR2: {PolRegR2}, PolRegR2Adj: {PolRegR2Adj}')\n",
    "print(f'RFR_RMSE: {RFR_RMSE}, RFR_R2: {RFR_R2}, RFR_R2Adj: {RFR_R2Adj}')\n",
    "print(f'XGB_RMSE: {XGB_RMSE}, XGB_R2: {XGB_R2}, XGB_R2Adj: {XGB_R2Adj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: pink;\">\n",
    "<h2>Interpretation:</h2>\n",
    "<p>I used RMSE and R2. </p>\n",
    "\n",
    "<p>Mean Squared Error depicts more error than there actually is due to squaring the errors.Mean Absolute Error weighs all errors equally which can be an issue since we have a range of different selling prices.</p>\n",
    "\n",
    "<p>Root Mean Squared Error doesn't have the issue that MSE does. It also is able to point out large errors. Since the data is scaled from 0 to 1, a RMSE over 1 is incredibly high. R2 compares the model against a constant. It doesn't account for the number of features Adjusted R2 adjusts for the amount of features. </p>\n",
    "\n",
    "<p>The Polynomial Regression model doesn't appear to be able to explain any of the data, based on the negative R2 and adjusted R2.</p>\n",
    "\n",
    "<p>Based on the Adjusted R2, the model that explains the most of the data is the Random Forest Regression. It has an Adjusted R2 of 0.469, explaining 46.9% of the data.  Random Forest Regression also has the lowest Root Mean Square Error, which indicates less error than the other models.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - STRETCH\n",
    "\n",
    "> **This step doesn't need to be part of your Minimum Viable Product (MVP), but its recommended you complete it if you have time!**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
